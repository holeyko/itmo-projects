{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from math import exp\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "np.seterr(over='ignore')\n",
    "\n",
    "Methods = Enum('Methods', ['Classic', 'Momentum', 'AdaGrad', 'RMSprop', 'Adam', 'Nesterov'])\n",
    "Regularization = Enum('Regularization', ['WithoutRegularization', 'L1', 'L2', 'Elastic'])\n",
    "LearningRate = Enum('LearningRate', ['Const'])\n",
    "LearningRateScheduling = Enum('LearningRateScheduling', ['Classic', 'Stepwise', 'Exponential'])\n",
    "\n",
    "\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, T, W, X, Y, regularization=Regularization.WithoutRegularization, l1=0.1, l2=0.1):\n",
    "        self.T = np.array([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))]).reshape(len(X), len(T))\n",
    "        self.W = W\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.regularization = regularization\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.W_points = [np.copy(self.W)]\n",
    "        self.loss_values = [self.loss(self.W)]\n",
    "\n",
    "    def loss(self, W_Arg, is_avarage=False):\n",
    "        val = sum([(np.dot(self.T[i], W_Arg) - self.Y[i]) ** 2 for i in range(len(self.X))])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                val += self.l1 * sum([abs(w) for w in self.W])\n",
    "            case Regularization.L2:\n",
    "                val += self.l2 * sum([w ** 2 for w in self.W])\n",
    "            case Regularization.Elastic:\n",
    "                val += (self.l1 * sum([abs(w) for w in self.W])) + (self.l2 * sum([w ** 2 for w in self.W]))\n",
    "\n",
    "        return val / len(self.X) if is_avarage else val\n",
    "\n",
    "    def grad_by_components(self, index_components, W_Arg):\n",
    "        grad_with_batch = np.zeros(len(W_Arg))\n",
    "        for i in index_components:\n",
    "            grad_with_batch += (2 * (np.dot(self.T[i], W_Arg) - self.Y[i]) * self.T[i])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                grad_with_batch += self.l1 * np.array([sign(w) for w in self.W])\n",
    "            case Regularization.L2:\n",
    "                grad_with_batch += self.l2 * 2 * self.W\n",
    "            case Regularization.Elastic:\n",
    "                grad_with_batch += (self.l1 * np.array([sign(w) for w in self.W])) + (self.l2 * 2 * self.W)\n",
    "\n",
    "        return grad_with_batch\n",
    "\n",
    "    def analytical_solution(self):\n",
    "        return (np.linalg.inv(np.transpose(self.T) @ self.T) @ np.transpose(self.T)) @ self.Y\n",
    "\n",
    "\n",
    "def sgd(lin_reg, lr, lrs, batch, max_num_of_step, beta_1, beta_2, eps_adam, is_corr_beta_1=True,\n",
    "        is_corr_beta_2=True, is_nesterov=False, is_adagrad=False, store_points=False):\n",
    "    i = -1\n",
    "    V = np.zeros(len(lin_reg.W))\n",
    "    S = np.zeros(len(lin_reg.W))\n",
    "    lrs_func = lrs_handler(lrs)\n",
    "    while True:\n",
    "        i += 1\n",
    "\n",
    "        components = [(i * batch + j) % len(lin_reg.X) for j in range(batch)]\n",
    "        cur_w = lin_reg.W\n",
    "        grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        alpha = lrs_func(lr(lambda a: lin_reg.loss(lin_reg.W - a * grad_with_batch)), (i * batch) // len(lin_reg.X))\n",
    "        if is_nesterov:\n",
    "            cur_w -= alpha * beta_1 * V\n",
    "            grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        V = (beta_1 * V) + (1 - beta_1) * grad_with_batch\n",
    "        S = (beta_2 * S) + (1 - beta_2) * (grad_with_batch ** 2) if ~is_adagrad else (S + grad_with_batch ** 2)\n",
    "        V_norm = V / (1 - beta_1 ** (i + 1)) if is_corr_beta_1 else V\n",
    "        S_norm = S / (1 - beta_2 ** (i + 1)) if is_corr_beta_2 else S\n",
    "\n",
    "        lin_reg.W = lin_reg.W - alpha * (V_norm / ((S_norm + eps_adam) ** 0.5))\n",
    "\n",
    "        loss_W = lin_reg.loss(lin_reg.W)\n",
    "        if store_points:\n",
    "            lin_reg.W_points.append(np.copy(lin_reg.W))\n",
    "        lin_reg.loss_values.append(loss_W)\n",
    "        if i >= max_num_of_step:\n",
    "            break\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def sgd_handler(lin_reg, lr, method, lrs=LearningRateScheduling.Classic, batch=1, beta_1=0.9, beta_2=0.999,\n",
    "                eps_adam=10 ** -8, max_num_of_step=5000, store_points=False):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return sgd(lin_reg, lr, lrs, batch, max_num_of_step, beta_1=0., beta_2=1., eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, store_points=store_points)\n",
    "        case Methods.Momentum:\n",
    "            return sgd(lin_reg, lr, lrs, batch, max_num_of_step, beta_1, beta_2=1., eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, store_points=store_points)\n",
    "        case Methods.AdaGrad:\n",
    "            return sgd(lin_reg, lr, lrs, batch, max_num_of_step, beta_1=0., beta_2=0.5, eps_adam=eps_adam,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, is_adagrad=True, store_points=store_points)\n",
    "        case Methods.RMSprop:\n",
    "            return sgd(lin_reg, lr, lrs, batch, max_num_of_step, beta_1=0., beta_2=beta_2, eps_adam=eps_adam,\n",
    "                       is_corr_beta_1=False, store_points=store_points)\n",
    "        case Methods.Adam:\n",
    "            return sgd(lin_reg, lr, lrs, batch, max_num_of_step, beta_1, beta_2, eps_adam,\n",
    "                       store_points=store_points)\n",
    "        case Methods.Nesterov:\n",
    "            return sgd(lin_reg, lr, lrs, batch, max_num_of_step, beta_1, beta_2=1., eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, is_nesterov=True, store_points=store_points)\n",
    "\n",
    "\n",
    "def lrs_exp(decay):\n",
    "    return lambda lr, epoch: lr * exp(-decay * epoch)\n",
    "\n",
    "\n",
    "def lrs_step(decay, epoch_update):\n",
    "    return lambda lr, epoch: lr * (decay ** (epoch // epoch_update))\n",
    "\n",
    "\n",
    "def lrs_handler(lrs, epoch_update=10):\n",
    "    match lrs:\n",
    "        case LearningRateScheduling.Classic:\n",
    "            return lambda lr, epoch: lr\n",
    "        case LearningRateScheduling.Stepwise:\n",
    "            return lrs_step(0.75, epoch_update)\n",
    "        case LearningRateScheduling.Exponential:\n",
    "            return lrs_exp(0.1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T20:43:54.761496Z",
     "end_time": "2023-04-18T20:43:54.902140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualise_points(lin_reg, need_analys_solve=True):\n",
    "    x = np.linspace(min(lin_reg.X), max(lin_reg.X), 1000)\n",
    "    y = sum([lin_reg.W[i] * (x ** i) for i in range(len(lin_reg.W))])\n",
    "    if need_analys_solve:\n",
    "        analys_w = lin_reg.analytical_solution()\n",
    "        analys_y = sum([analys_w[i] * (x ** i) for i in range(len(analys_w))])\n",
    "        plt.plot(x, analys_y, '-b')\n",
    "    plt.plot(x, y, '-r')\n",
    "    plt.plot(lin_reg.X, lin_reg.Y, 'og', linestyle='None')\n",
    "    plt.legend(['Analytics solution', 'Predict solution', 'Train data'])\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualise_linear_sgd(lin_reg):\n",
    "    values = [[], []]\n",
    "    for i in range(len(lin_reg.W_points)):\n",
    "        values[0].append(lin_reg.W_points[i][0])\n",
    "        values[1].append(lin_reg.W_points[i][1])\n",
    "    print(values[0][0], values[1][0])\n",
    "    X = np.linspace(min(values[0]) - 20, max(values[0]) + 20, 100)\n",
    "    Y = np.linspace(min(values[1]) - 20, max(values[1]) + 20, 100)\n",
    "    Z = [[lin_reg.loss(np.array([X[i], Y[j]])) for i in range(len(X))] for j in range(len(Y))]\n",
    "    plt.contour(X, Y, Z, 50)\n",
    "\n",
    "    plt.plot(values[0], values[1], marker='.')\n",
    "    plt.plot(values[0][0], values[1][0], 'og')\n",
    "    plt.plot(values[0][-1], values[1][-1], 'or')\n",
    "    plt.legend(['SGD', 'Start point', 'End point'])\n",
    "    plt.xlabel('w_1')\n",
    "    plt.ylabel('w_2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualise_batch_res(batch_res):\n",
    "    fig, ax = plt.subplots()\n",
    "    x = [i + 1 for i in range(len(batch_res) - 1)]\n",
    "    y = batch_res[1:]\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xticks(np.arange(1, len(batch_res), 10))\n",
    "    ax.set_xlabel('Batch size')\n",
    "    ax.set_ylabel('Relative error')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T20:43:54.954394Z",
     "end_time": "2023-04-18T20:43:54.990387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "from excel import ExcellSaver\n",
    "\n",
    "\n",
    "def poly_array(coeffs):\n",
    "    return [lambda x, i=i: coeffs[i] * (x ** i) for i in range(len(coeffs))]\n",
    "\n",
    "\n",
    "def poly(poly_arr):\n",
    "    return lambda x: sum([poly_arr[i](x) for i in range(len(poly_arr))])\n",
    "\n",
    "\n",
    "def generate_data(num_of_points, dimension, coeffs_left, coeffs_right, x_left, x_right, deviation, own_coeffs=None):\n",
    "    coeffs = own_coeffs\n",
    "    if coeffs == None:\n",
    "        coeffs = np.array([float(random.randint(coeffs_left, coeffs_right)) for i in range(dimension + 1)])\n",
    "\n",
    "    X = [random.uniform(x_left, x_right) for _ in range(num_of_points)]\n",
    "    Y = [poly(poly_array(coeffs))(X[i]) + random.uniform(-deviation, +deviation) for i in range(num_of_points)]\n",
    "\n",
    "    return [np.array(X), np.array(Y), coeffs]\n",
    "\n",
    "\n",
    "def gen_linear_reg(dimension, num_of_points, coeffs_left, coeffs_right, x_left, x_right, deviation, own_coeffs=None):\n",
    "    T = np.array(poly_array(np.ones(dimension + 1)))\n",
    "    X, Y, coeffs = generate_data(num_of_points, dimension, coeffs_left, coeffs_right, x_left, x_right, deviation,\n",
    "                                 own_coeffs)\n",
    "    W = np.ones(len(coeffs))\n",
    "\n",
    "    return LinearRegression(T, W, X, Y)\n",
    "\n",
    "\n",
    "def test_universal(lin_reg, lr, method, lrs, batch=1, store_points=False):\n",
    "    res_univ = {\n",
    "        'mem': 0,\n",
    "        'steps': 0,\n",
    "        'time': 0,\n",
    "        'error': 0\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "    steps = sgd_handler(lin_reg, lr, method, lrs=lrs, batch=batch, store_points=store_points)\n",
    "    res_univ['mem'] = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    end = time.time()\n",
    "\n",
    "    res_univ['steps'] = steps\n",
    "    res_univ['time'] = end - start\n",
    "    res_univ['error'] = lin_reg.loss(lin_reg.W)\n",
    "\n",
    "    return res_univ\n",
    "\n",
    "\n",
    "def refresh_lin_reg(lin_reg, own_W=None):\n",
    "    W = own_W\n",
    "    if W is None:\n",
    "        W = np.ones(len(lin_reg.W))\n",
    "\n",
    "    lin_reg.W = np.copy(W)\n",
    "    lin_reg.loss_values = [lin_reg.loss(lin_reg.W)]\n",
    "    lin_reg.W_points = [np.copy(lin_reg.W)]\n",
    "\n",
    "\n",
    "def show_info(method, step, lrs, batch, results):\n",
    "    info = 'Method: {} | LR : {}, LRS: {}, Batch: {} | Steps: {}, Time (sec): {}, Error: {} | Mem: {}' \\\n",
    "        .format(method.name, step, lrs.name, batch, results['steps'], results['time'], results['error'], results['mem'])\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def batch_test():\n",
    "    print('-' * 50)\n",
    "    print('Batch Test')\n",
    "\n",
    "    num_of_points = 60\n",
    "    lin_reg = gen_linear_reg(dimension=1, num_of_points=num_of_points,\n",
    "                             coeffs_left=-2, coeffs_right=2, x_left=0., x_right=1., deviation=0.5)\n",
    "\n",
    "    method = Methods.Classic\n",
    "    step = 0.01\n",
    "    lr = lambda *args: step\n",
    "    lrs = LearningRateScheduling.Classic\n",
    "\n",
    "    batch_res = [0]\n",
    "    for batch in range(1, num_of_points + 1):\n",
    "        refresh_lin_reg(lin_reg)\n",
    "        results = test_universal(lin_reg, lr, method, lrs, batch)\n",
    "        show_info(method, step, lrs, batch, results)\n",
    "        batch_res.append(results['error'] / lin_reg.loss(lin_reg.analytical_solution()))\n",
    "        if batch in [1, 2, 5, 10, num_of_points // 3, num_of_points]:\n",
    "            visualise_points(lin_reg)\n",
    "\n",
    "    visualise_batch_res(batch_res)\n",
    "\n",
    "\n",
    "def lrs_test():\n",
    "    print('-' * 50)\n",
    "    print('LRS Test')\n",
    "\n",
    "    num_of_points = 60\n",
    "    method = Methods.Classic\n",
    "    step = 0.01\n",
    "    lr = lambda *args: step\n",
    "    for lrs in LearningRateScheduling:\n",
    "        for i in range(3):\n",
    "            lin_reg = gen_linear_reg(dimension=1, num_of_points=num_of_points,\n",
    "                                     coeffs_left=-2, coeffs_right=2, x_left=0., x_right=1., deviation=1.)\n",
    "\n",
    "            refresh_lin_reg(lin_reg)\n",
    "            results = test_universal(lin_reg, lr, method, lrs)\n",
    "            show_info(method, step, lrs, results)\n",
    "            visualise_points(lin_reg)\n",
    "\n",
    "\n",
    "def methods_test():\n",
    "    print('-' * 50)\n",
    "    print('Method Test')\n",
    "\n",
    "    num_of_points = 60\n",
    "    step = 0.01\n",
    "    lr = lambda *args: step\n",
    "    lrs = LearningRateScheduling.Classic\n",
    "    for method in Methods:\n",
    "        for batch in [1, 5, num_of_points // 3, num_of_points]:\n",
    "            for i in range(2):\n",
    "                lin_reg = gen_linear_reg(dimension=1, num_of_points=num_of_points,\n",
    "                                         coeffs_left=-2, coeffs_right=2, x_left=0., x_right=1., deviation=0.5)\n",
    "                refresh_lin_reg(lin_reg)\n",
    "                results = test_universal(lin_reg, lr, method, lrs, batch, store_points=False)\n",
    "                show_info(method, step, lrs, batch, results)\n",
    "                visualise_points(lin_reg)\n",
    "\n",
    "\n",
    "def convergence_test():\n",
    "    saver.add_new_sheet(['Method', 'LR', 'LRS', 'Batch', 'Error', 'Time (sec)', 'Mem'], 'Convergence Test')\n",
    "    print('-' * 50)\n",
    "    print('Convergence Test')\n",
    "\n",
    "    num_of_points = 60\n",
    "    lin_reg = gen_linear_reg(dimension=1, num_of_points=num_of_points,\n",
    "                             coeffs_left=1, coeffs_right=2, x_left=1., x_right=2., deviation=0.5)\n",
    "    step = 0.01\n",
    "    lr = lambda *args: step\n",
    "    for method in Methods:\n",
    "        for lrs in LearningRateScheduling:\n",
    "            for batch in [1, num_of_points // 3]:\n",
    "                refresh_lin_reg(lin_reg)\n",
    "                results = test_universal(lin_reg, lr, method, lrs, batch, store_points=True)\n",
    "                saver.add_row([method.name, step, lrs.name, batch, results['error'], results['time'], results['mem']])\n",
    "                show_info(method, step, lrs, batch, results)\n",
    "\n",
    "\n",
    "def descent_trajectory_test():\n",
    "    print('-' * 50)\n",
    "    print('Descent Trajectory Test')\n",
    "\n",
    "    num_of_points = 60\n",
    "    lin_reg = gen_linear_reg(dimension=1, num_of_points=num_of_points,\n",
    "                             coeffs_left=-2, coeffs_right=2, x_left=0., x_right=1., deviation=0.5)\n",
    "    step = 0.01\n",
    "    lr = lambda *args: step\n",
    "    lrs = LearningRateScheduling.Classic\n",
    "    for method in Methods:\n",
    "        for batch in [1, num_of_points // 3, num_of_points]:\n",
    "            refresh_lin_reg(lin_reg, own_W=np.array([-50., 10.]))\n",
    "            results = test_universal(lin_reg, lr, method, lrs, batch, store_points=True)\n",
    "            show_info(method, step, lrs, batch, results)\n",
    "            last_point = lin_reg.W_points[-1]\n",
    "            lin_reg.W_points = lin_reg.W_points[::num_of_points // batch]\n",
    "            lin_reg.W_points.append(last_point)\n",
    "            print(lin_reg.W_points[0])\n",
    "            visualise_points(lin_reg)\n",
    "            visualise_linear_sgd(lin_reg)\n",
    "\n",
    "\n",
    "def poly_test():\n",
    "    print('-' * 50)\n",
    "    print('Poly Test')\n",
    "\n",
    "    num_of_points = 100\n",
    "    step = 0.01\n",
    "    lr = lambda *args: step\n",
    "    lrs = LearningRateScheduling.Classic\n",
    "    for method in Methods:\n",
    "        for batch in [1, num_of_points // 3, num_of_points]:\n",
    "            for dimension in [2, 5, 8]:\n",
    "                lin_reg = gen_linear_reg(dimension=dimension, num_of_points=num_of_points,\n",
    "                                         coeffs_left=-2, coeffs_right=2, x_left=-1., x_right=1., deviation=1.)\n",
    "                results = test_universal(lin_reg, lr, method, lrs, batch)\n",
    "                show_info(method, step, lrs, batch, results)\n",
    "                visualise_points(lin_reg)\n",
    "\n",
    "\n",
    "def regul_test():\n",
    "    print('-' * 50)\n",
    "    print('Regul Test')\n",
    "\n",
    "    num_of_points = 6\n",
    "    step = 0.01\n",
    "    lr = lambda *args: step\n",
    "    lin_reg = gen_linear_reg(dimension=num_of_points, num_of_points=num_of_points,\n",
    "                             coeffs_left=-3., coeffs_right=3., x_left=1.1, x_right=1.8, deviation=5., own_coeffs=[-3., 2., -8., 1., -2., 1., 1.])\n",
    "\n",
    "    for regularization in Regularization:\n",
    "        for method in Methods:\n",
    "            for lrs in LearningRateScheduling:\n",
    "                if lrs == LearningRateScheduling.Exponential:\n",
    "                    continue\n",
    "                for batch in [1, 5]:\n",
    "                    refresh_lin_reg(lin_reg)\n",
    "                    lin_reg.regularization = regularization\n",
    "                    results = test_universal(lin_reg, lr, method, lrs, batch)\n",
    "                    show_info(method, step, lrs, batch, results)\n",
    "                    print('Regularization: {}'.format(regularization.name))\n",
    "                    visualise_points(lin_reg)\n",
    "\n",
    "\n",
    "def ultimate_test():\n",
    "    x = [\n",
    "        [-2., 2.],\n",
    "        [-1., 2.],\n",
    "        [-1., 1.],\n",
    "        [0., 2],\n",
    "        [0., 1.]\n",
    "    ]\n",
    "    for dimension in range(2, 11):\n",
    "        for num_of_points in [20, 50, 70, 100]:\n",
    "            step = 0.01\n",
    "            x_left = x[0][0]\n",
    "            x_right = x[0][1]\n",
    "            if dimension >= 3 and dimension <= 5:\n",
    "                x_left = [2][0]\n",
    "                x_right = [2][1]\n",
    "            elif dimension >= 6 and dimension <= 11:\n",
    "                x_left = [4][0]\n",
    "                x_right = [4][1]\n",
    "            lin_reg = gen_linear_reg(dimension=dimension, num_of_points=num_of_points, coeffs_left=-2., coeffs_right=2.,\n",
    "                                     x_left=x_left, x_right=x_right, deviation=1.)\n",
    "            lr = lambda *args: step\n",
    "            for method in Methods:\n",
    "                for lrs in LearningRateScheduling:\n",
    "                    for batch in [1, 5, num_of_points // 3, num_of_points]:\n",
    "                        refresh_lin_reg(lin_reg)\n",
    "                        results = test_universal(lin_reg, lr, method, lrs, batch)\n",
    "                        show_info(method, step, lrs, batch, results)\n",
    "                        print('Dimension: {} | Count points: {}'.format(dimension, num_of_points))\n",
    "                        visualise_points(lin_reg)\n",
    "\n",
    "\n",
    "saver = ExcellSaver()\n",
    "\n",
    "# batch_test()\n",
    "# lrs_test()\n",
    "# methods_test()\n",
    "# convergence_test()\n",
    "# descent_trajectory_test()\n",
    "poly_test()\n",
    "# regul_test()\n",
    "# ultimate_test()\n",
    "# saver.create_excel()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T23:29:31.525112Z",
     "end_time": "2023-04-16T23:30:35.980893Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
